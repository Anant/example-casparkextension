{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import mcnemar_table, mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from random import randint, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import seaborn as sns\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import PCA, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col, asc\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "    pd.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, corr_columns, method='pearson'):\n",
    "    vector_col = \"corr_features\"\n",
    "    assembler = VectorAssembler(inputCols=corr_columns, outputCol=vector_col)\n",
    "    df_vector = assembler.transform(df).select(vector_col)\n",
    "    matrix = Correlation.corr(df_vector, vector_col, method)\n",
    "\n",
    "    result = matrix.collect()[0][\"pearson({})\".format(vector_col)].values\n",
    "    return pd.DataFrame(result.reshape(-1, len(corr_columns)), columns=corr_columns, index=corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Model 1')\n",
    "    plt.xlabel('Model 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['dse'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS accelerate \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('accelerate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS diabetes \\\n",
    "                                   (Id int, timesPregnant int, plasmaGlucose int, bloodPressure int, \\\n",
    "                                   tricepThickness int, serumInsulin int, bmi float, diabetesPedegree float, \\\n",
    "                                   age int, label int, PRIMARY KEY (Id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/pima-indians-diabetes.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "i = 1\n",
    "for line in input_file:\n",
    "    iD = i\n",
    "    row = line.split(',')\n",
    "\n",
    "    query = \"INSERT INTO diabetes (Id, timesPregnant, plasmaGlucose, bloodPressure, \\\n",
    "                                   tricepThickness, serumInsulin, bmi, diabetesPedegree, \\\n",
    "                                   age, label)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (int(iD), int(row[0]), int(row[1]), int(row[2]), int(row[3]), int(row[4]), float(row[5]), float(row[6]), int(row[7]), int(row[8])))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "diabetesDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"diabetes\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (diabetesDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetesDF.schema.names)\n",
    "print([diabetesDF.where((col(c_name) == 0)).count() for c_name in diabetesDF.schema.names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesDF = diabetesDF.withColumn(\"bloodpressure\", F.when(F.col(\"bloodpressure\")==0, float(\"nan\")).otherwise(F.col(\"bloodpressure\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"plasmaglucose\", F.when(F.col(\"plasmaglucose\")==0, float(\"nan\")).otherwise(F.col(\"plasmaglucose\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"tricepthickness\", F.when(F.col(\"tricepthickness\")==0, float(\"nan\")).otherwise(F.col(\"tricepthickness\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"seruminsulin\", F.when(F.col(\"seruminsulin\")==0, float(\"nan\")).otherwise(F.col(\"seruminsulin\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"bmi\", F.when(F.col(\"bmi\")==0, float(\"nan\")).otherwise(F.col(\"bmi\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetesDF.schema.names)\n",
    "print([diabetesDF.where((col(c_name) == 0)).count() for c_name in diabetesDF.schema.names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "imputer.setInputCols([\"plasmaglucose\", \"bloodpressure\", \"bmi\", \"tricepthickness\", \"seruminsulin\"])\n",
    "imputer.setOutputCols([\"out_plasmaglucose\", \"out_bloodpressure\", \"out_bmi\", \"out_tricepthickness\", \"out_seruminsulin\"])\n",
    "model = imputer.fit(diabetesDF)\n",
    "#model.setInputCols([\"plasmaglucose\", \"bloodpressure\", \"bmi\"])\n",
    "diabetesDF_imputed = model.transform(diabetesDF)\n",
    "showDF(diabetesDF_imputed,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF = diabetesDF_imputed.filter(F.col(\"out_tricepthickness\") != float(\"nan\"))\n",
    "tempDF = diabetesDF_imputed.filter(F.col(\"out_seruminsulin\") != float(\"nan\"))\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (tempDF.count())\n",
    "\n",
    "showDF(tempDF,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['age', 'out_bloodpressure', 'out_bmi', 'diabetespedegree', 'out_plasmaglucose', 'out_seruminsulin', 'timespregnant', 'out_tricepthickness'],\n",
    "    outputCol='features', handleInvalid = \"keep\")\n",
    "\n",
    "dDF = assembler.transform(diabetesDF_imputed)\n",
    "showDF(dDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dDF.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "print (\"Train Dataframe Row Count: \")\n",
    "print (train.count())\n",
    "print (\"Test Datafram Row Count: \")\n",
    "print (test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "rf_model = rf.fit(train)\n",
    "\n",
    "rf_predictions = rf_model.transform(test)\n",
    "showDF(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "rf_score = evaluator.evaluate(rf_predictions)\n",
    "print(\"Test set accuracy for Random Forest Classifier = \" + str(rf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [8,5,4,2]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "mlp_model = mlp.fit(train)\n",
    "\n",
    "mlp_predictions = mlp_model.transform(test)\n",
    "showDF(mlp_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_f1 = evaluator.evaluate(mlp_predictions)\n",
    "print(\"Test set accuracy for Random Forest Classifier = \" + str(mlp_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = np.reshape(np.array(test.select(\"label\").collect()), (-1))\n",
    "y_rf_model = np.reshape(np.array(rf_predictions.select(\"prediction\").collect()), (-1))\n",
    "y_nb_model = np.reshape(np.array(mlp_predictions.select(\"prediction\").collect()), (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = mcnemar_table(y_target=y_target, y_model1=y_rf_model, y_model2=y_nb_model)\n",
    "print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(tb, classes=[\"Correct\",\"Wrong\"],\n",
    "                      title='MCNemar Table')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p = mcnemar(ary=tb, exact=True)\n",
    "\n",
    "print('chi-squared:', chi2)\n",
    "print('p-value:', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
