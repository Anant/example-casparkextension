{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from random import randint, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import seaborn as sns\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import PCA, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col, asc\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "    pd.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, corr_columns, method='pearson'):\n",
    "    vector_col = \"corr_features\"\n",
    "    assembler = VectorAssembler(inputCols=corr_columns, outputCol=vector_col)\n",
    "    df_vector = assembler.transform(df).select(vector_col)\n",
    "    matrix = Correlation.corr(df_vector, vector_col, method)\n",
    "\n",
    "    result = matrix.collect()[0][\"pearson({})\".format(vector_col)].values\n",
    "    return pd.DataFrame(result.reshape(-1, len(corr_columns)), columns=corr_columns, index=corr_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dselogo.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['dse'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Demo Keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS accelerate \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('accelerate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `iris`. Our PRIMARY will be a unique key (status_id) we generate for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS iris \\\n",
    "                                   (Id int, SepalLengthCm float, SepalWidthCm float, \\\n",
    "                                   PetalLengthCm float, PetalWidthCm float, Species text, \\\n",
    "                                   PRIMARY KEY (Id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from CSV file\n",
    "\n",
    "#### Insert all the Data into the Apache Cassandra table `iris`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/datasets_19_420_Iris.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "next(input_file)\n",
    "for line in input_file:\n",
    "    row = line.split(',')\n",
    "\n",
    "    query = \"INSERT INTO iris (Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (int(row[0]), float(row[1]), float(row[2]), float(row[3]), float(row[4]), str(row[5])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading with Apache Spark\n",
    "<img src=\"images/sparklogo.png\" width=\"150\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "\n",
    "\n",
    "irisDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"iris\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (irisDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(irisDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"species\", outputCol=\"label\", handleInvalid='keep')\n",
    "training = labelIndexer.fit(irisDF).transform(irisDF)\n",
    "\n",
    "showDF(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisPD = training.toPandas()\n",
    "sns.countplot(y=irisPD.label)\n",
    "plt.xlabel(\"Count of each Target class\")\n",
    "plt.ylabel(\"Target classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['petallengthcm', 'sepalwidthcm', 'petalwidthcm', 'sepallengthcm'],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingData = assembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = trainingData.randomSplit([0.8, 0.2], 124)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "print (\"Train Dataframe Row Count: \")\n",
    "print (train.count())\n",
    "print (\"Test Datafram Row Count: \")\n",
    "print (test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "model = rf.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "#predictions.show()\n",
    "showDF(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"Test set f1 score = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(predictions, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "grid = ParamGridBuilder().addGrid(rf.numTrees, [5,15]).build()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=grid, evaluator=evaluator,\n",
    "    parallelism=2, numFolds = 5)\n",
    "cvModel = cv.fit(trainingData)\n",
    "print(cv.getNumFolds())\n",
    "print(cvModel.avgMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Folds\n",
    "Accuracy [0.9469198790627362, 0.9350151171579743]\n",
    "F1 Score [0.9468202574624536, 0.9347017340401462]\n",
    "\n",
    "5-Folds\n",
    "Accuracy [0.9672969966629588, 0.9605990783410137]\n",
    "F1 Score [0.9672383456365659, 0.9608919863240464]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS linear \\\n",
    "                                   (Id int, x float, y float, PRIMARY KEY (Id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/lin_test.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "next(input_file)\n",
    "i = 1\n",
    "iD = i\n",
    "for line in input_file:\n",
    "    row = line.split(',')\n",
    "\n",
    "    query = \"INSERT INTO linear (Id, x, y)\"\n",
    "    query = query + \" VALUES (%s, %s, %s)\"\n",
    "    session.execute(query, (int(iD),float(row[0]), float(row[1])))\n",
    "    i = i + 1\n",
    "    iD = iD + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/lin_train.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "next(input_file)\n",
    "i = 1\n",
    "for line in input_file:\n",
    "    row = line.split(',')\n",
    "\n",
    "    query = \"INSERT INTO linear (Id, x, y)\"\n",
    "    query = query + \" VALUES (%s, %s, %s)\"\n",
    "    #print(row)\n",
    "    if len(row)==2:\n",
    "        session.execute(query, (int(iD),float(row[0]), float(row[1])))\n",
    "        iD = iD + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "\n",
    "\n",
    "linDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"linear\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (linDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(linDF,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linDF = linDF.drop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linDF = linDF.withColumn(\"label\", linDF[\"y\"])\n",
    "\n",
    "showDF(linDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['x'],\n",
    "    outputCol='features')\n",
    "\n",
    "linDF = assembler.transform(linDF)\n",
    "showDF(linDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "splits = linDF.randomSplit([1/k]*k)\n",
    "showDF(splits[0])\n",
    "print (\"Split 0 Row Count: \"+str(splits[0].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2 = []\n",
    "rmse = []\n",
    "test_split_nums = []\n",
    "evaluator1 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "for i in range(k):\n",
    "    test = splits[i]\n",
    "    #print(test.count())\n",
    "    test_splits = list(range(k))\n",
    "    test_splits.remove(int(i))\n",
    "    #print(test_splits)\n",
    "    if k == 2:\n",
    "        train = splits[test_splits[0]]\n",
    "    elif k == 3:\n",
    "        train = splits[test_splits[0]].union(splits[test_splits[1]])\n",
    "    else:\n",
    "        train = splits[test_splits[0]].union(splits[test_splits[1]])\n",
    "        for j in range(k-3):\n",
    "            train = train.union(splits[test_splits[j+2]])\n",
    "    #print(train.count())\n",
    "    #showDF(train)\n",
    "    lr = LinearRegression(labelCol=\"label\", featuresCol=\"features\",maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "    model = lr.fit(train)\n",
    "    predictions = model.transform(test)\n",
    "    r2_val = evaluator1.evaluate(predictions)\n",
    "    rmse_val = evaluator2.evaluate(predictions)\n",
    "    r2.append(r2_val)\n",
    "    rmse.append(rmse_val)\n",
    "    test_split_nums.append(i)\n",
    "    #showDF(predictions)\n",
    "\n",
    "print(test_split_nums)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(test_split_nums)):\n",
    "    results.append({\"Test Split Number\":test_split_nums[i], \"R2\":r2[i], \"RMSE\":rmse[i]})\n",
    "print(results)\n",
    "resultsDF = spark.createDataFrame(pyspark.sql.Row(**x) for x in results)\n",
    "showDF(resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(predictions.select(\"x\").toPandas(), predictions.select(\"y\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predictions.select(\"x\").toPandas(), predictions.select(\"prediction\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions.select(\"y\").toPandas(), '.', predictions.select(\"prediction\").toPandas(), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
