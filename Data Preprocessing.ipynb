{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cassandra\n",
    "import pyspark\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from random import randint, randrange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import seaborn as sns\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import PCA, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, asc\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to have nicer formatting of Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper for pretty formatting for Spark DataFrames\n",
    "def showDF(df, limitRows =  5, truncate = True):\n",
    "    if(truncate):\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "    else:\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "    pd.set_option('display.max_rows', limitRows)\n",
    "    display(df.limit(limitRows).toPandas())\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, corr_columns, method='pearson'):\n",
    "    vector_col = \"corr_features\"\n",
    "    assembler = VectorAssembler(inputCols=corr_columns, outputCol=vector_col)\n",
    "    df_vector = assembler.transform(df).select(vector_col)\n",
    "    matrix = Correlation.corr(df_vector, vector_col, method)\n",
    "\n",
    "    result = matrix.collect()[0][\"pearson({})\".format(vector_col)].values\n",
    "    return pd.DataFrame(result.reshape(-1, len(corr_columns)), columns=corr_columns, index=corr_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dselogo.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables and Loading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['dse'])\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Demo Keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS accelerate \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set keyspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('accelerate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table called `socialMedia`. Our PRIMARY will be a unique key (status_id) we generate for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS iris \\\n",
    "                                   (Id int, SepalLengthCm float, SepalWidthCm float, \\\n",
    "                                   PetalLengthCm float, PetalWidthCm float, Species text, \\\n",
    "                                   PRIMARY KEY (Id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these columns represent: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from CSV file (socialMedia.csv)\n",
    "\n",
    "#### Insert all the Data into the Apache Cassandra table `iris`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/datasets_19_420_Iris.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "next(input_file)\n",
    "for line in input_file:\n",
    "    row = line.split(',')\n",
    "\n",
    "    query = \"INSERT INTO iris (Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, Species)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (int(row[0]), float(row[1]), float(row[2]), float(row[3]), float(row[4]), str(row[5])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading with Apache Spark\n",
    "<img src=\"images/sparklogo.png\" width=\"150\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('demo').master(\"local\").getOrCreate()\n",
    "\n",
    "\n",
    "irisDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"iris\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (irisDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(irisDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"species\", outputCol=\"label\", handleInvalid='keep')\n",
    "training = labelIndexer.fit(irisDF).transform(irisDF)\n",
    "\n",
    "showDF(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisPD = training.toPandas()\n",
    "sns.countplot(y=irisPD.label)\n",
    "plt.xlabel(\"Count of each Target class\")\n",
    "plt.ylabel(\"Target classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=sns.pairplot(irisPD, hue = 'species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "r1 = correlation_matrix(training, ['id', 'petallengthcm', 'sepalwidthcm', 'petalwidthcm', 'sepallengthcm', 'label'])\n",
    "p=sns.heatmap(r1, annot=True,cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['petallengthcm', 'sepalwidthcm', 'petalwidthcm', 'sepallengthcm'],\n",
    "    outputCol='features')\n",
    "\n",
    "trainingData = assembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = trainingData.randomSplit([0.8, 0.2], 124)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "print (\"Train Dataframe Row Count: \")\n",
    "print (train.count())\n",
    "print (\"Test Datafram Row Count: \")\n",
    "print (test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=4, inputCol=\"features\", outputCol=\"pca\")\n",
    "model = pca.fit(train)\n",
    "transformed = model.transform(train)\n",
    "showDF(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    plt.bar(range(4), model.explainedVariance.array, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "model = pca.fit(train)\n",
    "transformed_train = model.transform(train)\n",
    "transformed_test = model.transform(test)\n",
    "showDF(transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "model = rf.fit(transformed_train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "#predictions.show()\n",
    "showDF(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pca = RandomForestClassifier(labelCol=\"label\", featuresCol=\"pca\", numTrees=10)\n",
    "\n",
    "model_pca = rf_pca.fit(transformed_train)\n",
    "\n",
    "predictions_pca = model_pca.transform(transformed_test)\n",
    "#predictions.show()\n",
    "showDF(predictions_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_pca)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE IF NOT EXISTS diabetes \\\n",
    "                                   (Id int, timesPregnant int, plasmaGlucose int, bloodPressure int, \\\n",
    "                                   tricepThickness int, serumInsulin int, bmi float, diabetesPedegree float, \\\n",
    "                                   age int, label int, PRIMARY KEY (Id))\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'data/pima-indians-diabetes.csv'\n",
    "input_file = open(fileName, 'r')\n",
    "i = 1\n",
    "for line in input_file:\n",
    "    iD = i\n",
    "    row = line.split(',')\n",
    "\n",
    "    query = \"INSERT INTO diabetes (Id, timesPregnant, plasmaGlucose, bloodPressure, \\\n",
    "                                   tricepThickness, serumInsulin, bmi, diabetesPedegree, \\\n",
    "                                   age, label)\"\n",
    "    query = query + \" VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "    session.execute(query, (int(iD), int(row[0]), int(row[1]), int(row[2]), int(row[3]), int(row[4]), float(row[5]), float(row[6]), int(row[7]), int(row[8])))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesDF = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"diabetes\", keyspace=\"accelerate\").load()\n",
    "\n",
    "print (\"Table Row Count: \")\n",
    "print (diabetesDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(diabetesDF, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetesDF.schema.names)\n",
    "print([diabetesDF.where((col(c_name) == 0)).count() for c_name in diabetesDF.schema.names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetesDF = diabetesDF.withColumn(\"bloodpressure\", F.when(F.col(\"bloodpressure\")==0, float(\"nan\")).otherwise(F.col(\"bloodpressure\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"plasmaglucose\", F.when(F.col(\"plasmaglucose\")==0, float(\"nan\")).otherwise(F.col(\"plasmaglucose\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"tricepthickness\", F.when(F.col(\"tricepthickness\")==0, float(\"nan\")).otherwise(F.col(\"tricepthickness\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"seruminsulin\", F.when(F.col(\"seruminsulin\")==0, float(\"nan\")).otherwise(F.col(\"seruminsulin\")))\n",
    "diabetesDF = diabetesDF.withColumn(\"bmi\", F.when(F.col(\"bmi\")==0, float(\"nan\")).otherwise(F.col(\"bmi\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetesDF.schema.names)\n",
    "print([diabetesDF.where((col(c_name) == 0)).count() for c_name in diabetesDF.schema.names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showDF(diabetesDF, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer()\n",
    "imputer.setInputCols([\"plasmaglucose\", \"bloodpressure\", \"bmi\"])\n",
    "imputer.setOutputCols([\"out_plasmaglucose\", \"out_bloodpressure\", \"out_bmi\"])\n",
    "model = imputer.fit(diabetesDF)\n",
    "#model.setInputCols([\"plasmaglucose\", \"bloodpressure\", \"bmi\"])\n",
    "showDF(model.transform(diabetesDF),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
